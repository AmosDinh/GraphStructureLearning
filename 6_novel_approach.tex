
\chapter{Discussion}
\label{ch:discussion}
The results in Chapter \ref{ch:experimental} show that the GNN-based systems can significantly outperform the factorization based approach. Furthermore, the baselines are unable to generalize as seen in the validation loss and test MRR. The results show that the added continuous numerical features increase the baselines' performance. The significant performance differences highlight the benefit of the GNN-based approach. Unlike in the baselines' case, where features such as the skills of a job are arbitrarily aggregated by taking their mean, the GNN, and especially the attention based HGT can choose to weigh the importance of neighbors differently, and aggregate depending on the neighborhood context. This illustrates that for this particular problem, manual feature engineering and experimentation would be required for the FM to achieve comparable performance to the GNN-based approach. Compared to tabular data, the graph can model the relationships between entities allowing the GNN to consider the most important relationships. 

It is further investigated if a deeper GNNs would increase performance, as it was hypothesized that the four hops would allow the model the access to the skills of peoples' neighbors' jobs. However, the results show that even a two-layer-deep GNN has sufficient performance. Comparing the two and three-layer HGT, the three-layer HGT has a higher MRR, but also the mean rank is lower. Therefore the three-layer model may have overfit sections of the graph, on which it generates more accurate predictions compared to other regions, whereas the two-layer model performs more consistently. The observation also aligns with other work such as \parencite{ying2018graph} and \parencite{hamilton2017inductive} which report diminishing performance-returns for increased layer counts beyond two for user-item interaction settings. In a different setting like fraud detection in financial systems, one could imagine more complex entity interactions to be descriptive and therefore the necessity for deeper networks could be given. 

Whereas relatively, the GNN-based approach outperforms the baselines, no claim towards the general quality of the GNN-based recommendations can be made. Explainability frameworks such as the one proposed by \textcite{amara2022graphframex} can help to trace the exact neighbors which contributed to the scores calculated by the GNN. This might help to uncover unwated biases which could be of ethical nature or caused by customer specific graph structures, which could also explain the large standard deviation in the test results. But in general, the large performance differences between certain edges has to be investigated in detail, as consistent performance is crucial to a recommender system. Additionally, the explainability frameworks could help supervisors in the decision-making process of assigning specific learnings or creating learning content which appeals to employees. 

Practically speaking, maintaining the data in a graph allows to formulate all other link prediction tasks as well. In the Human Resource setting, this could for example allow matching candidates to job offerings. Once the graph is created, the experimental iteration is straightforward. \textcite{you2020design} show that the best GNN designs are highly task specific. Therefore, the hyperparameter space of the recommender system architecture should be further explored to increase performance. 

By training the GNN on general link prediction it is shown that the model still exhibits considerable performance. The embeddings represent both the entity interactions between different entity types in the graph as well as feature aggregations. Since the TransE method embeds all nodes in the same embedding space, node embeddings obtained this way can easily be utilized in further downstream tasks in the Human Resource context, like matching candidates to job positions by comparing their similarity in the embedding space. 





